{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Johnston et al. (2024) - Secondary thalamic dysfunction underlies abnormal large-scale neural dynamics in chronic stroke\n",
    "\n",
    "This notebook contains Python and R code used to generate the statistical results in Johnston et al. (2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Python setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt # Uncomment to install python dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load master_df_model (contains both spectral param info as well as 1-40 Hz full CT model sim parameters)\n",
    "region_df = pd.read_csv('data/MEG_params_by_region.csv')\n",
    "\n",
    "# Load thalamus measures (volume, MD, CBF) and degeneration score\n",
    "thal_df = pd.read_csv('data/thalamus_quant.csv')\n",
    "\n",
    "# Subcortical ROIs (to be excluded)\n",
    "subcorts = ['Hippocampus_L', 'Hippocampus_R', 'Amygdala_L', 'Amygdala_R', 'Caudate_L', 'Caudate_R', 'Putamen_L', 'Putamen_R', 'Pallidum_L', 'Pallidum_R', 'Thalamus_L', 'Thalamus_R']\n",
    "\n",
    "patients = ['P' + str(n) for n in np.arange(1,19)]\n",
    "controls = ['P' + str(n) for n in np.arange(1,24)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjohnston\\Anaconda3\\envs\\stroke_eeg2\\lib\\site-packages\\rpy2\\robjects\\packages.py:367: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Enable R \n",
    "os.environ['R_HOME'] = \"C:/Users/pjohnston/AppData/Local/Programs/R/R-4.3.1/\" # Path to your R installation with the packages below installed\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- \u001b[1mAttaching core tidyverse packages\u001b[22m ---------------------------------------------------------------- tidyverse 2.0.0 --\n",
      "\u001b[32mv\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.2     \u001b[32mv\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32mv\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32mv\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32mv\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.2     \u001b[32mv\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32mv\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32mv\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32mv\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.1     \n",
      "-- \u001b[1mConflicts\u001b[22m ---------------------------------------------------------------------------------- tidyverse_conflicts() --\n",
      "\u001b[31mx\u001b[39m \u001b[34mtidyr\u001b[39m::\u001b[32mexpand()\u001b[39m masks \u001b[34mMatrix\u001b[39m::expand()\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31mx\u001b[39m \u001b[34mtidyr\u001b[39m::\u001b[32mpack()\u001b[39m   masks \u001b[34mMatrix\u001b[39m::pack()\n",
      "\u001b[31mx\u001b[39m \u001b[34mtidyr\u001b[39m::\u001b[32munpack()\u001b[39m masks \u001b[34mMatrix\u001b[39m::unpack()\n",
      "\u001b[36mi\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading required package: Matrix\n",
       "\n",
       "Attaching package: 'lmerTest'\n",
       "\n",
       "The following object is masked from 'package:lme4':\n",
       "\n",
       "    lmer\n",
       "\n",
       "The following object is masked from 'package:stats':\n",
       "\n",
       "    step\n",
       "\n",
       "Loading required package: MASS\n",
       "\n",
       "Attaching package: 'MASS'\n",
       "\n",
       "The following object is masked from 'package:dplyr':\n",
       "\n",
       "    select\n",
       "\n",
       "Loading required package: mvtnorm\n",
       "Loading required package: sandwich\n",
       "mediation: Causal Mediation Analysis\n",
       "Version: 4.5.0\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library('lme4')\n",
    "library('tidyverse')\n",
    "library('lmeresampler')\n",
    "library('lmerTest')\n",
    "library('mediation')\n",
    "library('dplyr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_by_controls(df, column_name):\n",
    "    \n",
    "    mean = np.mean(df[column_name][df.group=='control'])\n",
    "    std = np.std(df[column_name][df.group=='control'])\n",
    "    \n",
    "    normalized_values = (df[column_name] - mean) / std\n",
    "    \n",
    "    return normalized_values\n",
    "\n",
    "def compute_pca(df):\n",
    "    # Impute missing values, standardize columns, compute PCA. Returns PCA and computed scores.\n",
    "    \n",
    "    # Perform imputation\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    # Perform column normalization\n",
    "    scaler = StandardScaler()\n",
    "    df_normalized = pd.DataFrame(scaler.fit_transform(df_imputed), columns=df_imputed.columns)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(df_normalized)\n",
    "    \n",
    "    scores = pca.transform(df_normalized)\n",
    "    \n",
    "    return pca, scores\n",
    "\n",
    "def mean_and_sd(data, round_digits):\n",
    "    mean = np.round(np.mean(data), round_digits) \n",
    "    sd = np.round(np.std(data), round_digits)\n",
    "    \n",
    "    return mean, sd\n",
    "\n",
    "def cohen_d(d1, d2):\n",
    "    # Compute Cohen's d \n",
    "\n",
    "     # calculate the size of samples\n",
    "     n1, n2 = len(d1), len(d2)\n",
    "     # calculate the variance of the samples\n",
    "     s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    "     # calculate the pooled standard deviation\n",
    "     s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "     # calculate the means of the samples\n",
    "     u1, u2 = np.mean(d1), np.mean(d2)\n",
    "     # calculate the effect size\n",
    "     return (u1 - u2) / s\n",
    "\n",
    "def CI_ind(d1, d2, percent):\n",
    "    # Calculate the given confidence interval for two independent samples\n",
    "\n",
    "    outer_margin=(1-percent)/2\n",
    "    \n",
    "    N1 = len(d1)\n",
    "    N2 = len(d2)\n",
    "    df = (N1 + N2 - 2)\n",
    "    std1 = np.std(d1)\n",
    "    std2 = np.std(d2)\n",
    "    std_N1N2 = np.sqrt( ((N1 - 1)*(std1)**2 + (N2 - 1)*(std2)**2) / df) \n",
    "\n",
    "    diff_mean = np.mean(d1) - np.mean(d2)\n",
    "    MoE = t.ppf(1-outer_margin, df) * std_N1N2 * np.sqrt(1/N1 + 1/N2)\n",
    "    \n",
    "    return [diff_mean - MoE, diff_mean + MoE]\n",
    "\n",
    "    \n",
    "def pearsonr_CI(r, n):\n",
    "    # Compute the 95% confidence interval for a Pearson correlation coefficient.\n",
    "\n",
    "    # Fisher's z transformation\n",
    "    z = np.arctanh(r)\n",
    "    \n",
    "    # Standard error of Fisher's z\n",
    "    se = 1 / np.sqrt(n - 3)\n",
    "    \n",
    "    # Margin of error at 95% confidence level (z-score of 1.96)\n",
    "    margin_error = 1.96 * se\n",
    "    \n",
    "    # Compute confidence interval for Fisher's z\n",
    "    lower_z = z - margin_error\n",
    "    upper_z = z + margin_error\n",
    "    \n",
    "    # Transform back to Pearson correlation scale\n",
    "    lower_r = np.tanh(lower_z)\n",
    "    upper_r = np.tanh(upper_z)\n",
    "    \n",
    "    return lower_r, upper_r\n",
    "\n",
    "\n",
    "def report_group_diff(d1, d2, labels=[]):\n",
    "    print(labels[0] + \": \" + str(mean_and_sd(d1, 3)))\n",
    "    print(labels[1] + \": \" + str(mean_and_sd(d2, 3)))\n",
    "    print(scipy.stats.ttest_ind(d1,d2))\n",
    "    print(\"Cohen's d: \" + str(cohen_d(d1, d2)))\n",
    "    print('95% CI: ' + str(CI_ind(d1, d2, 0.95)))\n",
    "    print('\\n')\n",
    "\n",
    "def compute_GLMs(df):\n",
    "    # Compute GLM between each pair of columns in given df\n",
    "\n",
    "    # Loop through each pair of columns\n",
    "    for i, col1 in enumerate(df.columns):\n",
    "        for j, col2 in enumerate(df.columns):\n",
    "            if i < j:  # Ensure only unique pairs are considered\n",
    "                d1 = df[col1]\n",
    "                d2 = df[col2]\n",
    "\n",
    "                # Remove nans from input\n",
    "                nans = np.logical_or(np.isnan(d1), np.isnan(d2))\n",
    "                d1 = d1[~nans]\n",
    "                d2 = d2[~nans]\n",
    "\n",
    "                # Standardize input\n",
    "                d1 = [(x-np.mean(d1))/np.std(d1) for x in d1]\n",
    "                d2 = [(x-np.mean(d2))/np.std(d2) for x in d2]\n",
    "\n",
    "                # Fit linear model\n",
    "                glm_fit = sm.GLM(d1, d2).fit()\n",
    "                slope = glm_fit.params[0]\n",
    "                p_value = glm_fit.pvalues[0]\n",
    "\n",
    "                # Compute CI\n",
    "                l_ci, u_ci = pearsonr_CI(slope, len(d1))\n",
    "\n",
    "                print(col1 + ' ~ ' + col2)\n",
    "                print('beta = ' + str(np.round(slope,3)))\n",
    "                print('p = ' + str(np.round(p_value,3)))\n",
    "                print('95% CI [' + str(np.round(l_ci, 4)) + ',' + str(np.round(u_ci, 4)) + ']')\n",
    "                #print(scipy.stats.pearsonr(d1,d2))\n",
    "                print('\\n')\n",
    "                \n",
    "                glm_fit.summary()\n",
    "\n",
    "                \n",
    "def residualize(x, y, z):\n",
    "    # Returns the residuals of x and y after regressing on z. Removes any row with nans from all three variables.\n",
    "    \n",
    "    # Create a mask to filter out NaN values in either X or Y\n",
    "    mask = ~np.isnan(x) & ~np.isnan(y) & ~np.isnan(z)\n",
    "\n",
    "    # Apply the mask to X, Y, and Z\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "    z = z[mask]\n",
    "\n",
    "    data = pd.DataFrame({'x': x, 'y': y, 'z' : z})\n",
    "\n",
    "    # Regression of X on Z\n",
    "    model_x = smf.ols(formula='x~z', data=data)\n",
    "    results_x = model_x.fit()\n",
    "    residuals_x = results_x.resid\n",
    "\n",
    "    # Regression of Y on Z\n",
    "    model_y = smf.ols(formula='y~z', data=data)\n",
    "    results_y = model_y.fit()\n",
    "    residuals_y = results_y.resid\n",
    "    \n",
    "    return residuals_x, residuals_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare subject-wise dataframe\n",
    "Compute average left hemisphere, right hemisphere, and difference (left minus right) values for parameters of interest, add to dataframe.\n",
    "\n",
    "Only average over regions with spheres that are >=50% intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters of interest\n",
    "params = ['z', 'x', 'y', 'Gee', 'Gei', 'Gese', 'Gesre', 'Gsrs', 'alpha', 'beta', 't0', 'EMG', 'chisq', 'exp', 'ta_cf', 'ta_pw', 'ta_bw', 'b_cf', 'b_pw', 'b_bw', 'offset']\n",
    "\n",
    "subj_df = thal_df # Create new subj-wise df\n",
    "\n",
    "for param in params:\n",
    "\n",
    "    lvals = []\n",
    "    rvals = []\n",
    "    diff_vals = []\n",
    "\n",
    "    for subj in subj_df.subj:\n",
    "\n",
    "        # Take only cortical ROIS with spheres >=50% intact\n",
    "        trim_df = region_df[(region_df.subj == subj) & (~region_df.roi_label.isin(subcorts)) & (region_df.percent_sphere_in_lesion < 0.5)] \n",
    "        \n",
    "        lmean = np.mean(trim_df[param][trim_df.hemi == 'L'])\n",
    "        rmean = np.mean(trim_df[param][trim_df.hemi == 'R'])\n",
    "        \n",
    "        lvals.append(lmean)\n",
    "        rvals.append(rmean)\n",
    "        diff_vals.append(lmean-rmean)\n",
    "\n",
    "    \n",
    "    # Add lists of vals to dataframe\n",
    "    subj_df['lcort_' + param] = lvals\n",
    "    subj_df['rcort_' + param] = rvals\n",
    "    subj_df['diff_' + param] = diff_vals\n",
    "\n",
    "# Normalize left and right hemisphere values based on controls ('cnorm')\n",
    "new_columns = []\n",
    "for param in params:\n",
    "    lcort_col = 'lcort_' + param + '_cnorm'\n",
    "    rcort_col = 'rcort_' + param + '_cnorm'\n",
    "    \n",
    "    lcort_norm = normalize_column_by_controls(subj_df, 'lcort_' + param)\n",
    "    rcort_norm = normalize_column_by_controls(subj_df, 'rcort_' + param)\n",
    "    \n",
    "    new_columns.extend([(lcort_col, lcort_norm), (rcort_col, rcort_norm)])\n",
    "\n",
    "subj_df = pd.concat([subj_df, pd.DataFrame.from_dict(dict(new_columns))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Corticothalamic model fitting\n",
    "\n",
    "Compare model goodness of fit between patients and controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi square, L hemi average (mean, sd):\n",
      "Patients: (0.546, 0.157)\n",
      "Controls: (0.664, 0.191)\n",
      "Ttest_indResult(statistic=-2.0788620322816382, pvalue=0.044254774643103616)\n",
      "Cohen's d: -0.654210372064878\n",
      "95% CI: [-0.23172798513529386, -0.00600065619735729]\n",
      "\n",
      "\n",
      "Chi square, R hemi average (mean, sd):\n",
      "Patients: (0.61, 0.201)\n",
      "Controls: (0.632, 0.177)\n",
      "Ttest_indResult(statistic=-0.3763048182107243, pvalue=0.7087297753604778)\n",
      "Cohen's d: -0.11842176696124869\n",
      "95% CI: [-0.1425079452086266, 0.09683453004883466]\n",
      "\n",
      "\n",
      "Likelihood, whole brain (mean, sd):\n",
      "Patient both hemis: (0.752, 0.066)\n",
      "Control both hemis: (0.726, 0.066)\n",
      "Ttest_indResult(statistic=1.7588907325782865, pvalue=0.0824198773834603)\n",
      "\n",
      "\n",
      "Likelihood, L hemi (mean, sd):\n",
      "Patients: (0.764, 0.058)\n",
      "Controls: (0.721, 0.067)\n",
      "Ttest_indResult(statistic=2.112306188514975, pvalue=0.04111613367338227)\n",
      "Cohen's d: 0.6647351272208512\n",
      "95% CI: [0.0028221154541105548, 0.08315287752891191]\n",
      "\n",
      "\n",
      "Likelihood, R hemi (mean, sd):\n",
      "Patients: (0.741, 0.071)\n",
      "Controls: (0.732, 0.064)\n",
      "Ttest_indResult(statistic=0.42072323267154915, pvalue=0.6762666078727764)\n",
      "Cohen's d: 0.13240008153898655\n",
      "95% CI: [-0.03366797043355475, 0.051928629070481916]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = subj_df.lcort_chisq[(subj_df.group == 'patient')]\n",
    "d2 = subj_df.lcort_chisq[(subj_df.group == 'control')]\n",
    "print(\"Chi square, L hemi average (mean, sd):\")\n",
    "report_group_diff(d1,d2, labels=['Patients', 'Controls'])\n",
    "\n",
    "\n",
    "d1 = subj_df.rcort_chisq[(subj_df.group == 'patient')]\n",
    "d2 = subj_df.rcort_chisq[(subj_df.group == 'control')]\n",
    "print(\"Chi square, R hemi average (mean, sd):\")\n",
    "report_group_diff(d1,d2, labels=['Patients', 'Controls'])\n",
    "\n",
    "\n",
    "# Calculate likelihood estimate as BrainTrak does and add to subj_df\n",
    "subj_df['lcort_chisq_lk'] = [math.exp(-chisq/2) for chisq in subj_df['lcort_chisq']]\n",
    "subj_df['rcort_chisq_lk'] = [math.exp(-chisq/2) for chisq in subj_df['rcort_chisq']]\n",
    "\n",
    "\n",
    "print(\"Likelihood, whole brain (mean, sd):\")\n",
    "d1 = np.concatenate([subj_df.lcort_chisq_lk[subj_df.group == 'patient'], \n",
    "                     subj_df.rcort_chisq_lk[subj_df.group == 'patient']])\n",
    "d2 = np.concatenate([subj_df.lcort_chisq_lk[subj_df.group == 'control'], \n",
    "                     subj_df.rcort_chisq_lk[subj_df.group == 'control']])           \n",
    "print(\"Patient both hemis: \" + str(mean_and_sd(d1, 3)))\n",
    "print(\"Control both hemis: \" + str(mean_and_sd(d2, 3)))\n",
    "print(scipy.stats.ttest_ind(d1, d2))\n",
    "print('\\n')\n",
    "\n",
    "# Left hemisphere patients vs controls\n",
    "d1=subj_df.lcort_chisq_lk[(subj_df.group == 'patient')]\n",
    "d2=subj_df.lcort_chisq_lk[(subj_df.group == 'control')]\n",
    "print(\"Likelihood, L hemi (mean, sd):\")\n",
    "report_group_diff(d1,d2, labels=['Patients', 'Controls'])\n",
    "\n",
    "# Right hemisphere patients vs controls\n",
    "d1=subj_df.rcort_chisq_lk[(subj_df.group == 'patient')]\n",
    "d2=subj_df.rcort_chisq_lk[(subj_df.group == 'control')]\n",
    "print(\"Likelihood, R hemi (mean, sd):\")\n",
    "report_group_diff(d1,d2, labels=['Patients', 'Controls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Stroke patients demonstrate decreased intrathalamic inhibition in the ipsilesional hemisphere\n",
    "\n",
    "Examine group differences in the hemisphere-averaged model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x lcort (mean, sd):\n",
      "Patients: (0.617, 0.055)\n",
      "Controls: (0.592, 0.064)\n",
      "Ttest_indResult(statistic=1.2569800097405508, pvalue=0.2162369970324673)\n",
      "Cohen's d: 0.3955670684638644\n",
      "95% CI: [-0.013905829347930297, 0.06269045019238653]\n",
      "\n",
      "\n",
      "y lcort (mean, sd):\n",
      "Patients: (0.106, 0.035)\n",
      "Controls: (0.1, 0.031)\n",
      "Ttest_indResult(statistic=0.5905480338380207, pvalue=0.5582301500713693)\n",
      "Cohen's d: 0.18584333300624467\n",
      "95% CI: [-0.014592277968964808, 0.027067611687784206]\n",
      "\n",
      "\n",
      "z lcort (mean, sd):\n",
      "Patients: (0.137, 0.034)\n",
      "Controls: (0.214, 0.048)\n",
      "Ttest_indResult(statistic=-5.65620767540552, pvalue=1.5548623135354315e-06)\n",
      "Cohen's d: -1.7799881234744435\n",
      "95% CI: [-0.10377385041097131, -0.050064642283882466]\n",
      "\n",
      "\n",
      "Gee lcort (mean, sd):\n",
      "Patients: (12.768, 0.752)\n",
      "Controls: (12.812, 0.855)\n",
      "Ttest_indResult(statistic=-0.1670280102887737, pvalue=0.8682110507049614)\n",
      "Cohen's d: -0.052563111480921514\n",
      "95% CI: [-0.5601777595175395, 0.47275567599114465]\n",
      "\n",
      "\n",
      "Gei lcort (mean, sd):\n",
      "Patients: (-19.996, 1.409)\n",
      "Controls: (-20.85, 1.411)\n",
      "Ttest_indResult(statistic=1.8755527233237155, pvalue=0.06821727725273728)\n",
      "Cohen's d: 0.590229666952071\n",
      "95% CI: [-0.044222456298904755, 1.7509187713562762]\n",
      "\n",
      "\n",
      "Gese lcort (mean, sd):\n",
      "Patients: (8.869, 2.166)\n",
      "Controls: (11.33, 3.242)\n",
      "Ttest_indResult(statistic=-2.7037304705355267, pvalue=0.010106233424238121)\n",
      "Cohen's d: -0.8508542123648534\n",
      "95% CI: [-4.25869087350124, -0.6633129854860098]\n",
      "\n",
      "\n",
      "Gesre lcort (mean, sd):\n",
      "Patients: (-3.785, 1.291)\n",
      "Controls: (-4.289, 1.35)\n",
      "Ttest_indResult(statistic=1.1790326890887584, pvalue=0.2455283053866248)\n",
      "Cohen's d: 0.37103732822463387\n",
      "95% CI: [-0.3392399261155592, 1.3467548363565915]\n",
      "\n",
      "\n",
      "Gsrs lcort (mean, sd):\n",
      "Patients: (-1.49, 0.394)\n",
      "Controls: (-2.392, 0.568)\n",
      "Ttest_indResult(statistic=5.599953675199596, pvalue=1.8609368701008731e-06)\n",
      "Cohen's d: 1.7622851928165277\n",
      "95% CI: [0.5840928825465236, 1.2204642151031058]\n",
      "\n",
      "\n",
      "alpha lcort (mean, sd):\n",
      "Patients: (65.655, 7.938)\n",
      "Controls: (64.234, 8.37)\n",
      "Ttest_indResult(statistic=0.5382172682344284, pvalue=0.5934856954465748)\n",
      "Cohen's d: 0.1693750301057407\n",
      "95% CI: [-3.788556090745497, 6.630762636832695]\n",
      "\n",
      "\n",
      "beta lcort (mean, sd):\n",
      "Patients: (533.449, 33.631)\n",
      "Controls: (551.111, 35.168)\n",
      "Ttest_indResult(statistic=-1.5865120410332436, pvalue=0.12069905469359779)\n",
      "Cohen's d: -0.4992696083398124\n",
      "95% CI: [-39.626115272385476, 4.302702201240411]\n",
      "\n",
      "\n",
      "t0 lcort (mean, sd):\n",
      "Patients: (0.098, 0.008)\n",
      "Controls: (0.093, 0.006)\n",
      "Ttest_indResult(statistic=1.9443017527085444, pvalue=0.05909746548338002)\n",
      "Cohen's d: 0.6118647381566694\n",
      "95% CI: [-5.591021608658368e-05, 0.008243577259546912]\n",
      "\n",
      "\n",
      "EMG lcort (mean, sd):\n",
      "Patients: (0.115, 0.044)\n",
      "Controls: (0.086, 0.026)\n",
      "Ttest_indResult(statistic=2.5283012059048433, pvalue=0.015621195324221725)\n",
      "Cohen's d: 0.7956472564904687\n",
      "95% CI: [0.006397511119131279, 0.0515068172698476]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose hemisphere (lcort or rcort)\n",
    "hemi = 'lcort' \n",
    "\n",
    "params_to_test = ['x', 'y', 'z', 'Gee', 'Gei', 'Gese', 'Gesre', 'Gsrs', 'alpha', 'beta', 't0', 'EMG']\n",
    "\n",
    "for p in params_to_test:\n",
    "    \n",
    "    d1=subj_df[hemi + \"_\" + p][(subj_df.group == 'patient')]\n",
    "    d2=subj_df[hemi + \"_\" + p][(subj_df.group == 'control')]\n",
    "    print(p + \" \" +  hemi + \" (mean, sd):\")\n",
    "    report_group_diff(d1, d2, labels=['Patients', 'Controls'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson r =  -0.9017319975972135\n",
      "p =  3.199748647685364e-07\n",
      "95% CI: [-0.9631,-0.751]\n"
     ]
    }
   ],
   "source": [
    "### add Z and Gsrs correlation\n",
    "d1=subj_df[hemi + \"_z\"][(subj_df.group == 'patient')]\n",
    "d2=subj_df[hemi + \"_Gsrs\"][(subj_df.group == 'patient')]\n",
    "r, p = scipy.stats.pearsonr(d1, d2)\n",
    "l_ci, u_ci = pearsonr_CI(r, len(d1))\n",
    "print('Pearson r = ', str(r))\n",
    "print('p = ', str(p))\n",
    "print('95% CI: [' + str(np.round(l_ci, 4)) + ',' + str(np.round(u_ci, 4)) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Stroke patients show evidence of ipsilesional thalamic degeneration on multiple measures\n",
    "\n",
    "Independent samples t-tests (patients vs controls):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lthal_volume_cnorm lthal (mean, sd):\n",
      "Patients: (-2.091, 1.587)\n",
      "Controls: (0.0, 1.0)\n",
      "Ttest_indResult(statistic=-5.019447770574209, pvalue=1.1778438088713163e-05)\n",
      "Cohen's d: -1.579602081598216\n",
      "95% CI: [-2.9112639246504486, -1.2702070062770874]\n",
      "\n",
      "\n",
      "lthal_MD_cnorm lthal (mean, sd):\n",
      "Patients: (2.022, 2.076)\n",
      "Controls: (-0.0, 1.0)\n",
      "Ttest_indResult(statistic=3.9717300821884036, pvalue=0.0003070706812700775)\n",
      "Cohen's d: 1.2703440074501398\n",
      "95% CI: [1.020025040170334, 3.02324026321074]\n",
      "\n",
      "\n",
      "lthal_CBF_cnorm lthal (mean, sd):\n",
      "Patients: (-0.442, 1.875)\n",
      "Controls: (-0.0, 1.0)\n",
      "Ttest_indResult(statistic=-0.9269864480766222, pvalue=0.35994118707402456)\n",
      "Cohen's d: -0.3017739720755545\n",
      "95% CI: [-1.3793348846528182, 0.49588108432673106]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose thalamus (lthal or rthal)\n",
    "hemi = 'lthal' \n",
    "\n",
    "thal_params = [hemi+'_volume_cnorm', hemi+'_MD_cnorm', hemi+'_CBF_cnorm']\n",
    "\n",
    "for p in thal_params:\n",
    "    \n",
    "    d1=subj_df[p][(subj_df.group == 'patient')]\n",
    "    d1=d1.dropna()\n",
    "    d2=subj_df[p][(subj_df.group == 'control')]\n",
    "    d2=d2.dropna()\n",
    "    print(p + \" \" +  hemi + \" (mean, sd):\")\n",
    "    \n",
    "    report_group_diff(d1, d2, labels=['Patients', 'Controls'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression between thalamus measures (Figure 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lthal_volume_cnorm ~ lthal_MD_cnorm\n",
      "beta = -0.329\n",
      "p = 0.163\n",
      "95% CI [-0.6992,0.1799]\n",
      "\n",
      "\n",
      "lthal_volume_cnorm ~ lthal_CBF_cnorm\n",
      "beta = 0.404\n",
      "p = 0.087\n",
      "95% CI [-0.1151,0.7494]\n",
      "\n",
      "\n",
      "lthal_MD_cnorm ~ lthal_CBF_cnorm\n",
      "beta = -0.573\n",
      "p = 0.007\n",
      "95% CI [-0.8322,-0.1076]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glm_df = subj_df.loc[subj_df.group == 'patient', [hemi+'_volume_cnorm', hemi+'_MD_cnorm', hemi+'_CBF_cnorm']]\n",
    "\n",
    "compute_GLMs(glm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute PCA on thalamus measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC loadings [volume, MD, CBF]\n",
      "[[-0.50865915  0.59565035 -0.62166433]\n",
      " [-0.84798498 -0.47152384  0.24204699]\n",
      " [ 0.14895418 -0.65028143 -0.74494746]]\n",
      "\n",
      "\n",
      "Explained variance ratio (PC1, PC2, PC3)\n",
      "[0.62602351 0.23398508 0.13999141]\n"
     ]
    }
   ],
   "source": [
    "# Compute PCA with imputation and standardization\n",
    "pca, scores = compute_pca(subj_df.loc[subj_df.group == 'patient', [hemi+'_volume_cnorm', hemi+'_MD_cnorm', hemi+'_CBF_cnorm']])\n",
    "\n",
    "# Get components and invert so that higher score on PC1 means more degeneration \n",
    "components = pca.components_\n",
    "components_inv = components*-1\n",
    "\n",
    "# Print PCs and variance explained\n",
    "print('PC loadings [volume, MD, CBF]')\n",
    "print(components_inv)\n",
    "print('\\n')\n",
    "\n",
    "print('Explained variance ratio (PC1, PC2, PC3)')\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model-estimated intrathalamic inhibition links thalamus degeneration and abnormal MEG dynamics\n",
    "\n",
    "First compute MEG slowing score based on parameters identified in Johnston et al., 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA\n",
    "slowing_measures = ['lcort_exp_cnorm', 'lcort_ta_cf_cnorm', 'lcort_b_pw_cnorm']\n",
    "slowing_pca_df = subj_df.loc[subj_df.group == 'patient', slowing_measures]\n",
    "slowing_pca, slowing_pca_scores = compute_pca(slowing_pca_df)\n",
    "\n",
    "\n",
    "# Add scores to small dataframe for merging by subj number to subj_df\n",
    "lcort_slowing_scores_df = pd.DataFrame(data = {'subj' : subj_df.subj[subj_df.group == 'patient'], \n",
    "                                 'slowing_score_left' : slowing_pca_scores[:,0]})\n",
    "\n",
    "# Merge with subject-wise dataframe (if column doesn't already exist)\n",
    "if 'slowing_score_left' not in subj_df.keys():\n",
    "\n",
    "    subj_df = pd.merge(subj_df, lcort_slowing_scores_df, how='left', left_on='subj', right_on='subj')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute linear regression on each pair of: thalamus degeneration score, Z, and MEG slowing score, and lesion volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degen_score_left ~ lcort_z_cnorm\n",
      "beta = -0.551\n",
      "p = 0.006\n",
      "95% CI [-0.8097,-0.1136]\n",
      "\n",
      "\n",
      "degen_score_left ~ slowing_score_left\n",
      "beta = 0.456\n",
      "p = 0.035\n",
      "95% CI [-0.0144,0.7606]\n",
      "\n",
      "\n",
      "degen_score_left ~ lesion_volume\n",
      "beta = 0.084\n",
      "p = 0.729\n",
      "95% CI [-0.3988,0.5298]\n",
      "\n",
      "\n",
      "lcort_z_cnorm ~ slowing_score_left\n",
      "beta = -0.871\n",
      "p = 0.0\n",
      "95% CI [-0.9511,-0.6809]\n",
      "\n",
      "\n",
      "lcort_z_cnorm ~ lesion_volume\n",
      "beta = 0.163\n",
      "p = 0.495\n",
      "95% CI [-0.3285,0.5856]\n",
      "\n",
      "\n",
      "slowing_score_left ~ lesion_volume\n",
      "beta = -0.087\n",
      "p = 0.719\n",
      "95% CI [-0.5321,0.3961]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glm_df2 = subj_df.loc[subj_df.group == 'patient', ['degen_score_left', 'lcort_z_cnorm', 'slowing_score_left', 'lesion_volume']]\n",
    "\n",
    "compute_GLMs(glm_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute PCA on thalamus degeneration score, Z, and MEG slowing score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC loadings [volume, MD, CBF]\n",
      "[[ 0.48938008 -0.62748593  0.60561418]\n",
      " [-0.86458625 -0.25831671  0.43100244]\n",
      " [ 0.1140077   0.7345297   0.66893076]]\n",
      "\n",
      "\n",
      "Explained variance ratio (PC1, PC2, PC3)\n",
      "[0.75682618 0.20274444 0.04042938]\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "# Compute PCA with imputation and standardization\n",
    "pca, scores = compute_pca(subj_df.loc[subj_df.group == 'patient', ['degen_score_left', 'lcort_z_cnorm', 'slowing_score_left']])\n",
    "\n",
    "# Get components \n",
    "components = pca.components_\n",
    "\n",
    "# Print PCs and variance explained\n",
    "print('PC loadings [volume, MD, CBF]')\n",
    "print(components)\n",
    "print('\\n')\n",
    "\n",
    "print('Explained variance ratio (PC1, PC2, PC3)')\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "#subj_df.to_csv('subj_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Causal Mediation Analysis in R to determine whether Z mediates relationship between thalamus degeneration and MEG slowing.\n",
    "\n",
    "Note: p values and confidence intervals will vary due to bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Pass patient data to R\n",
    "\n",
    "patient_df = subj_df[subj_df.group == 'patient']\n",
    "\n",
    "%R -i patient_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Causal Mediation Analysis \n",
      "\n",
      "Nonparametric Bootstrap Confidence Intervals with the Percentile Method\n",
      "\n",
      "               Estimate 95% CI Lower 95% CI Upper p-value   \n",
      "ACME             0.4908       0.1955         0.91   0.002 **\n",
      "ADE             -0.0353      -0.3491         0.19   0.632   \n",
      "Total Effect     0.4555       0.0756         0.86   0.022 * \n",
      "Prop. Mediated   1.0775       0.7142         2.76   0.020 * \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Sample Size Used: 18 \n",
      "\n",
      "\n",
      "Simulations: 1000 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Running nonparametric bootstrap\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R \n",
    "\n",
    "patient_df_scaled = as.data.frame(scale(patient_df[c('degen_score_left', 'lcort_z_cnorm', 'slowing_score_left')]))\n",
    "\n",
    "model.M <- lm(lcort_z_cnorm ~ degen_score_left, patient_df_scaled) #M ~ X\n",
    "model.Y <- lm(slowing_score_left ~ degen_score_left + lcort_z_cnorm, patient_df_scaled) #Y ~ M + X\n",
    "med_results <- mediate(model.M, model.Y, treat='degen_score_left', mediator='lcort_z_cnorm',\n",
    "                   boot=TRUE, sims=1000)\n",
    "\n",
    "\n",
    "summary(med_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mediation Sensitivity Analysis for Average Causal Mediation Effect\n",
      "\n",
      "Sensitivity Region\n",
      "\n",
      "      Rho    ACME 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n",
      "[1,] -0.9 -0.1781      -0.3784       0.0223         0.81       0.1357\n",
      "[2,] -0.8  0.0588      -0.1009       0.2185         0.64       0.1072\n",
      "[3,] -0.7  0.1732      -0.0249       0.3713         0.49       0.0821\n",
      "\n",
      "Rho at which ACME = 0: -0.8\n",
      "R^2_M*R^2_Y* at which ACME = 0: 0.64\n",
      "R^2_M~R^2_Y~ at which ACME = 0: 0.1072 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "sens = medsens(med_results,\n",
    "               rho.by=0.1,\n",
    "               effect.type='indirect')\n",
    "\n",
    "summary(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Lesion proximity predicts MEG spectral slowing\n",
    "\n",
    "Use a linear mixed effects model in R to explore the effect of proximity to lesion on spectral slowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%R` not found.\n"
     ]
    }
   ],
   "source": [
    "# Load df with ipsilesional spectra/model parameters normalized by controls for each region\n",
    "roi_df_left = pd.read_csv('data/roi_df_left.csv')\n",
    "\n",
    "# Pass region-level data to R\n",
    "%R -i roi_df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n",
      "lmerModLmerTest]\n",
      "Formula: \n",
      "roi_slowing_pc_score ~ degen_score_left * prox_to_lesion + lesion_volume +  \n",
      "    (prox_to_lesion | subj)\n",
      "   Data: roi_df_left_scaled\n",
      "\n",
      "REML criterion at convergence: 1007.7\n",
      "\n",
      "Scaled residuals: \n",
      "    Min      1Q  Median      3Q     Max \n",
      "-3.7057 -0.5971  0.0333  0.5456  4.0057 \n",
      "\n",
      "Random effects:\n",
      " Groups   Name           Variance Std.Dev. Corr \n",
      " subj     (Intercept)    0.39228  0.6263        \n",
      "          prox_to_lesion 0.06609  0.2571   -0.09\n",
      " Residual                0.46325  0.6806        \n",
      "Number of obs: 447, groups:  subj, 18\n",
      "\n",
      "Fixed effects:\n",
      "                                Estimate Std. Error       df t value Pr(>|t|)\n",
      "(Intercept)                      0.04214    0.15491 15.14463   0.272 0.789259\n",
      "degen_score_left                 0.26651    0.14741 15.29084   1.808 0.090335\n",
      "prox_to_lesion                   0.36245    0.07647 15.72656   4.740 0.000232\n",
      "lesion_volume                   -0.16171    0.14373 15.36748  -1.125 0.277816\n",
      "degen_score_left:prox_to_lesion -0.05749    0.07740 19.74571  -0.743 0.466424\n",
      "                                   \n",
      "(Intercept)                        \n",
      "degen_score_left                .  \n",
      "prox_to_lesion                  ***\n",
      "lesion_volume                      \n",
      "degen_score_left:prox_to_lesion    \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Correlation of Fixed Effects:\n",
      "            (Intr) dgn_s_ prx_t_ lsn_vl\n",
      "dgn_scr_lft -0.159                     \n",
      "prox_to_lsn -0.108 -0.004              \n",
      "lesion_volm -0.013 -0.071 -0.074       \n",
      "dgn_scr_:__ -0.009 -0.113  0.001 -0.054\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# Scale numeric columns only \n",
    "roi_df_left_scaled = roi_df_left\n",
    "roi_df_left_scaled = roi_df_left_scaled %>% mutate(across(where(is.numeric), scale))\n",
    "\n",
    "# Create a normalized \"proximity to lesion\" measure by multiplying Z-scored distance by *-1\n",
    "roi_df_left_scaled$prox_to_lesion = -1*roi_df_left_scaled$dist_to_lesion\n",
    "\n",
    "# Mixed effects model with effect of left thalamus degen score (control normalized), distance to lesion, and their interaction (plus lesion size) with varying intercept and slope for each patient\n",
    "#slowing_model <- lmer(roi_slowing_pc_score ~ degen_score_excl3_left * prox_to_lesion + lesion_volume + (1|subj), data=roi_df_left_scaled)\n",
    "slowing_model <- lmer(roi_slowing_pc_score ~ degen_score_left * prox_to_lesion + lesion_volume + (prox_to_lesion|subj), data=roi_df_left_scaled)\n",
    "# Bootstrap slowing model\n",
    "slowing_model_boot <- bootstrap(slowing_model, .f=fixef, type='parametric', B=1000) # Only need to estimate the fixed effects for now\n",
    "\n",
    "summary(slowing_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Thalamic degeneration is correlated with cognitive and language impairment\n",
    "\n",
    "Compute Spearman correlation between thalamus degeneration and cognitive/language measures after controlling for effect of lesion size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load behavioural data\n",
    "behav_df = pd.read_csv('data/behav.csv')\n",
    "\n",
    "# Create new dataframe merging subj_df and behavioural measures\n",
    "corr_df = subj_df.loc[subj_df.group == 'patient', ['subj', 'degen_score_left', 'lcort_z', 'slowing_score_left', 'lesion_volume']].merge(behav_df, on='subj', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moca ~ thalamus degeneration\n",
      "Spearman r: -0.6868131868131868, p = 0.009509496050393583\n",
      "\n",
      "\n",
      "bnt ~ thalamus degeneration\n",
      "Spearman r: -0.5612745098039216, p = 0.019063465403200246\n",
      "\n",
      "\n",
      "ppvt ~ thalamus degeneration\n",
      "Spearman r: -0.556372549019608, p = 0.02037058662451899\n",
      "\n",
      "\n",
      "wab_aphasia_score ~ thalamus degeneration\n",
      "Spearman r: -0.2916666666666667, p = 0.2560026072263\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "behavs = ['moca', 'bnt', 'ppvt', 'wab_aphasia_score']\n",
    "\n",
    "# Compute Spearman correlations between thalamus degen and behavioural measures, after residualizing by lesion volume\n",
    "x = corr_df['degen_score_left']\n",
    "z = corr_df['lesion_volume']\n",
    "\n",
    "for b in behavs:\n",
    "\n",
    "    y = corr_df[b]\n",
    "    x_resid, y_resid = residualize(x,y,z) # Residualize on lesion volume\n",
    "\n",
    "    print(b + ' ~ thalamus degeneration')\n",
    "    r, p = scipy.stats.spearmanr(x_resid, y_resid)\n",
    "    print(f'Spearman r: {r}, p = {p}')\n",
    "    print('\\n')\n",
    "    \n",
    "    #plt.scatter(x_resid, y_resid)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check correlations between lesion volume and raw behavioural measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moca ~ lesion_volume\n",
      "Spearman r: -0.23933997882811733, p = 0.43095955006186404\n",
      "\n",
      "\n",
      "bnt ~ lesion_volume\n",
      "Spearman r: -0.2015149839839546, p = 0.4379920364155855\n",
      "\n",
      "\n",
      "ppvt ~ lesion_volume\n",
      "Spearman r: -0.31592036089432557, p = 0.21671971676993526\n",
      "\n",
      "\n",
      "wab_aphasia_score ~ lesion_volume\n",
      "Spearman r: -0.4757818184278815, p = 0.05355540911569535\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in behavs:\n",
    "\n",
    "    x = corr_df['lesion_volume']\n",
    "    y = corr_df[b]\n",
    "    \n",
    "    # Remove nans from input\n",
    "    nans = np.logical_or(np.isnan(x), np.isnan(y))\n",
    "    x = x[~nans]\n",
    "    y = y[~nans]\n",
    "    \n",
    "    print(b + ' ~ lesion_volume')\n",
    "    r, p = scipy.stats.spearmanr(x, y)\n",
    "    print(f'Spearman r: {r}, p = {p}')\n",
    "    print('\\n')\n",
    "    \n",
    "    #plt.scatter(x_resid, y_resid)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Material\n",
    "\n",
    "## S1 Thalamus degeneration and primary tissue damage differentially predict language impairment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    bnt   R-squared:                       0.429\n",
      "Model:                            OLS   Adj. R-squared:                  0.238\n",
      "Method:                 Least Squares   F-statistic:                     2.250\n",
      "Date:                Fri, 19 Jul 2024   Prob (F-statistic):              0.124\n",
      "Time:                        10:29:56   Log-Likelihood:                -19.365\n",
      "No. Observations:                  17   AIC:                             48.73\n",
      "Df Residuals:                      12   BIC:                             52.90\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                     -0.1103      0.225     -0.490      0.633      -0.601       0.380\n",
      "degen_score_excl3_left    -0.5523      0.233     -2.369      0.035      -1.060      -0.044\n",
      "Angular_L                 -0.1683      0.269     -0.626      0.543      -0.754       0.417\n",
      "Temporal_Sup_L            -0.0142      0.291     -0.049      0.962      -0.649       0.621\n",
      "IFG_L                     -0.3813      0.336     -1.135      0.279      -1.114       0.351\n",
      "==============================================================================\n",
      "Omnibus:                        1.768   Durbin-Watson:                   2.441\n",
      "Prob(Omnibus):                  0.413   Jarque-Bera (JB):                0.973\n",
      "Skew:                           0.585   Prob(JB):                        0.615\n",
      "Kurtosis:                       2.920   Cond. No.                         2.36\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Load patient info for regression\n",
    "reg_df = pd.read_csv('data/reg_df.csv')\n",
    "\n",
    "y = reg_df['bnt'].copy() # 'bnt', 'ppvt', or 'wab_aphasia_score'\n",
    "\n",
    "predictors = ['degen_score_excl3_left', 'Angular_L', 'Temporal_Sup_L', 'IFG_L']\n",
    "#['degen_score_excl3_left', 'lesion_volume'] + lang_regions\n",
    "X = reg_df[predictors].copy()\n",
    "\n",
    "\n",
    "# Normalize\n",
    "X = zscore(X, nan_policy='omit')\n",
    "y = zscore(y, nan_policy='omit')\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2, missing='drop')\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
